# -*- coding: utf-8 -*-
"""APAN5400-A5-hf2444.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FrJMizkk9nRyKdzAlaQWQjkBDIIFfD87
"""

!pip install pyspark

from pyspark.conf import SparkConf
from pyspark import SparkContext
from pyspark.sql import SQLContext

sc = SparkContext() 
config = sc.getConf()
config.set('spark.cores.max','4')
config.set('spark.executor.memory', '8G')
config.set('spark.driver.maxResultSize', '8g')
config.set('spark.kryoserializer.buffer.max', '512m')
config.set("spark.driver.cores", "4")

sc.stop()

sc = SparkContext(conf = config) 
sqlContext = SQLContext(sc)
print("Using Apache Spark Version", sc.version)

cb_file = "crunchbase_odm_orgs.csv"

cb_sdf = sqlContext.read.option("header", "true").option("delimiter", ",").option("inferSchema", "true").csv(cb_file)
cb_sdf.count()

cb_sdf = sqlContext.read.format("csv") \
                        .options(header='true', inferschema='true', treatEmptyValuesAsNulls='true') \
                        .load(cb_file)
cb_sdf.count()

cb_sdf.columns

cb_rdd = cb_sdf.select('*').rdd.map(lambda row: [str(row[i]) for i in ['uuid','name','domain','city']])
cb_sdf2 = sqlContext.createDataFrame(cb_rdd,['uuid','name','domain','city'])
cb_sdf2.show(10)

from pyspark.sql.functions import monotonically_increasing_id

cb_sdf3 = cb_sdf2.select("*").withColumn("index", monotonically_increasing_id())

# Apply transformation to name filed 
from pyspark.sql.functions import initcap
cb_sdf3 = cb_sdf3.withColumn('name_lower', initcap('name'))
cb_sdf3['name','name_lower'].show()

def upper_case(entity_name):
    return entity_name.upper()

from pyspark.sql.types import StringType
from pyspark.sql.functions import udf

spark_udf = udf(upper_case, StringType())
cb_sdf3 = cb_sdf3.withColumn('name_upper',spark_udf('name'))
cb_sdf3['name','name_lower','name_upper'].show(20, truncate=False)

#Q1 Find all entities with the name that starts with a letter "F" (e.g. Facebook, etc.)
df_temp = cb_sdf3.filter(cb_sdf3.name.startswith('F'))
print(df_temp.count())
df_temp.show()

#Q2 Find all entities located in New York City
df_temp2 = cb_sdf3.filter(cb_sdf3.city =="New York")
print(df_temp2.count())
df_temp2.show()

#Add a "Blog" column to the DataFrame with the row entries set to 1 if the "domain" field contains "blogspot.com", and 0 otherwise.
#show() only the records with the "Blog" field marked as 1
from pyspark.sql.functions import when
df_temp3=cb_sdf3.withColumn('blog',when(cb_sdf3.domain.like('%blogspot.com'),1).otherwise(0))
df_temp3.filter(df_temp3.blog == 1).show()

#Find all entities with names that are palindromes (name reads the same way forward and reverse, e.g. madam):
#print the count and show() the resulting Spark DataFrame 
from pyspark.sql.functions import *

def find_pali(val):
  if val[::-1].upper() == val.upper():
    return True
  else:
    return False

from pyspark.sql.functions import udf
from pyspark.sql.types import BooleanType

find_palindromes = udf(find_pali,BooleanType())

df_temp4 = cb_sdf3.select('*', find_palindromes(cb_sdf3.name_upper))
df_pali = df_temp4.filter(find_palindromes('name') == 'true')
df_pali.show()
print(df_pali.count())